seed: 777 # random seed
data_path: ./ # input data path
batch_size: 1 # train batch size
eval_batch_size: 1 # evaluation batch size
num_workers: 4 # number of subprocesses to use for data loading
max_epochs: 6 # number of maximum epochs
use_amp: false # use AMP (Automatic Mixed Precision)
debug: false
train_epoch_length: null
eval_epoch_length: null
filename_prefix: training
n_saved: 2                                                                         
save_every_iters: 300
patience: 30
output_dir: ./logs
log_every_iters: 10
model: WeOpenML/PandaLM-7B-v1 #microsoft/deberta-v3-base # microsoft/deberta-v3-xsmall # model for text classification
model_dir: /tmp/model # path of model directory
tokenizer_dir: /tmp/tokenizer # path of directory containing tokenizer
num_classes: 3 # number of classes
drop_out: .15 # drop out rate
n_fc: 768 # number of fully-connected layers
weight_decay: 0.01 # parameter for weight decay, default, 1e-4.
num_warmup_epochs: 0 # number of warm-up epochs before learning rate decay
max_length: 1024 # maximum length of texts
lr: 0.00001 #0.00005
device: cuda # device 
lora_rank: 4
lora_alpha: 8
lora_modules: ['q_proj','k_proj','o_proj', 'v_proj', 'down_proj', 'lm_head']
pad_token_id: 128001
checkpoint_fp: 